{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Note: this code is written as an assignment in a Helsinki Uni course on Deep Learning and is heavily influenced by\n",
    "#starter code provided by the lecturers Hande Celikkanat and Roman Yangarber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Got a working training session from start to finish\n",
    "#Good result but this used pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Downloading done\n",
      "Unzipping data...\n",
      "Unzipping done...\n"
     ]
    }
   ],
   "source": [
    "#Custom functions to read in our data from internet\n",
    "#Skips if data already exists\n",
    "\n",
    "from src import data_download\n",
    "data_download.fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#These are custom made functions to handle our data\n",
    "#Maybe more documentation later\n",
    "#The function used here can split our data to different sets\n",
    "\n",
    "from src import data_handling\n",
    "train, test, val = data_handling.get_target_dfs(train=0.6, test=0.2, val=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.dataset import CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform_mix = [transforms.ColorJitter(brightness=.5, hue=.3), transforms.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2)]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=CustomImageDataset(train, DATA_DIR, transform=transform_mix), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=CustomImageDataset(test, DATA_DIR, transform=None), batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=CustomImageDataset(val, DATA_DIR, transform=None), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The rest of the code is a pretty standard simple Pytorch setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cuda device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Found cuda device\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading our models\n",
    "from src.models import MultiLabelResnet, MultiLabelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comment out the model you don't want to use\n",
    "model = MultiLabelResnet().to(device)\n",
    "#model = MultiLabelCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This utility function is taken from Deep Learning course Programming Assignment 3\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log results:\n",
    "from src.log_results import Logger\n",
    "logger = Logger()\n",
    "logger.log(\"model_name\", model.__class__.__name__)\n",
    "logger.log(\"model_str\", model.__str__())\n",
    "logger.log(\"optimizer\", optimizer.__str__())\n",
    "logger.log(\"loss_function\", loss_function.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1...\n",
      "Epoch: 01 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.888 | Train Acc: 81.15%\n",
      "\t Val. Loss: 0.823 |  Val. Acc: 92.82%\n",
      "Starting Epoch 2...\n",
      "Epoch: 02 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.764 | Train Acc: 92.78%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 92.87%\n",
      "Starting Epoch 3...\n",
      "Epoch: 03 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.717 | Train Acc: 92.79%\n",
      "\t Val. Loss: 0.712 |  Val. Acc: 93.03%\n",
      "Starting Epoch 4...\n",
      "Epoch: 04 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.703 | Train Acc: 92.95%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 93.23%\n",
      "Starting Epoch 5...\n",
      "Epoch: 05 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.698 | Train Acc: 93.44%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 93.73%\n",
      "Starting Epoch 6...\n",
      "Epoch: 06 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.695 | Train Acc: 93.74%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 93.82%\n",
      "Starting Epoch 7...\n",
      "Epoch: 07 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.693 | Train Acc: 93.87%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 93.94%\n",
      "Starting Epoch 8...\n",
      "Epoch: 08 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.692 | Train Acc: 93.94%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 94.02%\n",
      "Starting Epoch 9...\n",
      "Epoch: 09 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.691 | Train Acc: 93.99%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 94.01%\n",
      "Postponing early-stopping\n",
      "Starting Epoch 10...\n",
      "Epoch: 10 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.691 | Train Acc: 94.03%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 94.10%\n",
      "Starting Epoch 11...\n",
      "Epoch: 11 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.690 | Train Acc: 94.07%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 94.10%\n",
      "Postponing early-stopping\n",
      "Starting Epoch 12...\n",
      "Epoch: 12 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.690 | Train Acc: 94.08%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 94.14%\n",
      "Starting Epoch 13...\n",
      "Epoch: 13 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.690 | Train Acc: 94.10%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 94.30%\n",
      "Starting Epoch 14...\n",
      "Epoch: 14 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.689 | Train Acc: 94.12%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 94.22%\n",
      "Postponing early-stopping\n",
      "Starting Epoch 15...\n",
      "Epoch: 15 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.689 | Train Acc: 94.13%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 94.14%\n",
      "Breaking loop due to early-stopping\n"
     ]
    }
   ],
   "source": [
    "epochs=25\n",
    "early_stop_patience = 1 # How many epochs to go without improvement\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "postpone_early_stop = early_stop_patience\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    start_time = time.time()\n",
    "    model.train() #Enables dropout layer\n",
    "    print(f'Starting Epoch {epoch+1}...')\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for batch_num, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        predicted_labels = (outputs > 0.5).int()\n",
    "        train_accuracy += (predicted_labels == labels).float().mean().item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = train_accuracy / len(train_loader)\n",
    "    \n",
    "    ### VALIDATION\n",
    "    model.eval() #Disables dropout layer\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (inputs, labels) in enumerate(val_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted_labels = (outputs > 0.5).int()\n",
    "            val_accuracy += (predicted_labels == labels).float().mean().item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_accuracy / len(val_loader)\n",
    "    \n",
    "    logger.append(\"train_loss\", train_loss)\n",
    "    logger.append(\"train_accuracy\", train_accuracy)\n",
    "    logger.append(\"val_loss\", val_loss)\n",
    "    logger.append(\"val_accuracy\", val_accuracy)\n",
    "    \n",
    "    ### PRINTOUT\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    logger.append(\"epoch_time\", f\"{epoch_mins}m {epoch_secs}s\")\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_accuracy*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_accuracy*100:.2f}%')\n",
    "    \n",
    "    ### EARLY STOP\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        postpone_early_stop = early_stop_patience #Reset patience\n",
    "        torch.save(model.state_dict(), 'temp_best_model_state.pt') #Save best model state\n",
    "        logger.log(\"best_epoch\", epoch+1)\n",
    "        continue\n",
    "        \n",
    "    #allowing for some epoch to have worse accuracy than the one before\n",
    "    elif postpone_early_stop > 0:\n",
    "        postpone_early_stop -= 1 \n",
    "        print(\"Postponing early-stopping\")\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Breaking loop due to early-stopping\")\n",
    "        model.load_state_dict(torch.load('temp_best_model_state.pt')) #Load model state from best epoch\n",
    "        logger.log(\"early_stop\", True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.694\n",
      "Test Accuracy: 92.92%\n"
     ]
    }
   ],
   "source": [
    "#The output returns a probability array for every label\n",
    "#Probability is the probability of label=1 (image has the specific label)\n",
    "#These are turned to actual predictions with predicted_labels = (outputs > 0.5).int()\n",
    "#This means that if it's more likely than not that image has a certain label, then it gets assigned the label\n",
    "#Otherwise the image will not have the label\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval() #Disables dropout layer\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted_labels = (outputs > 0.5).int()\n",
    "        test_accuracy += (predicted_labels == labels).float().mean().item()\n",
    "\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = test_accuracy / len(test_loader)\n",
    "\n",
    "logger.log(\"test_loss\", test_loss)\n",
    "logger.log(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If good results, save model state and metadata:\n",
    "#Note: Overwrites results for same model\n",
    "#logger.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b00c71c3385904d38907326321bc6ca9930b9cd405025156e2f0e2cd3cac88e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
